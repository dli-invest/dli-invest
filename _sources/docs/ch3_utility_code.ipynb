{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(utility_code)=\n",
    "# Web Scrapping and Data Extraction\n",
    "\n",
    "Oftentimes, there is data available on the internet and/or in files in which you want specific parts. When I was reading finance books and technical books I found it incredibly useful to extract tables from pdfs. Below is the code I use to extract data, note that camelot requires (tk and ghostscript).\n",
    "\n",
    "% Add reference for camelot\n",
    "\n",
    "As someone who has gone through university using latex, it was only natural for me to want to convert content locked in pdf into formats useful to me.\n",
    "\n",
    "% Consider adding this code to appendix\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "extractTables.py\n",
    "    --- Takes tables from pdf documents and then outputs latex tables\n",
    "\"\"\"\n",
    "__author__ = \"David Li\"\n",
    "import os\n",
    "import argparse \n",
    "import pandas as pd\n",
    "import camelot\n",
    "import glob\n",
    "import io\n",
    "from os import makedirs, listdir, removedirs, chdir\n",
    "from os.path import isfile, exists, splitext\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 3000)\n",
    "# See https://gist.github.com/marianoguerra/1137302\n",
    "import sys\n",
    "import csv\n",
    "import pytablewriter\n",
    "\n",
    "def to_rst(csv_path, rst_path='default.tex'):\n",
    "    '''\n",
    "    to convert a csv table to rst\n",
    "    '''\n",
    "    try:\n",
    "        print(csv_path)\n",
    "        # process(in_=path, out=None, title=title)\n",
    "        writer = pytablewriter.RstGridTableWriter()\n",
    "        writer.table_name = \"example_table\"\n",
    "        rst_df = pd.read_csv(str(csv_path))\n",
    "        print(rst_df)\n",
    "        writer.from_dataframe(rst_df)\n",
    "        with open(rst_path, 'w') as rst_f:\n",
    "            rst_f.write(writer.dumps())\n",
    "    except:\n",
    "        print('Unable to write to file')\n",
    "\n",
    "def to_tex(csv_path, tex_path='default.tex'):\n",
    "    '''to convert a csv table to tex using pandas'''\n",
    "    try:\n",
    "        tex_df = pd.read_csv(str(csv_path))\n",
    "        tex_df.to_latex(tex_path,longtable=True)\n",
    "    except:\n",
    "        print('Unable to write to file')\n",
    "\n",
    "def mkdir_new(folder_name='2019-01-03'):\n",
    "    \"\"\"Makes a new directory if it doesn't already exist\"\"\"\n",
    "    if not exists(folder_name):\n",
    "        makedirs(folder_name)\n",
    "\n",
    "def main(args):\n",
    "    # In general I like hardcoding the file name\n",
    "    pdf_file = f'Stefan Jansen - Machine Learning for Algorithmic Trading_ Predictive models to extract signals from market and alternative data for systematic trading strategies with Python, 2nd Edition (2020, Packt Publishing) - li.pdf'\n",
    "    path_to_file = os.path.join('C:/', 'Users', 'stude', 'Desktop', pdf_file)\n",
    "    tables = camelot.read_pdf(path_to_file, pages='118,119')\n",
    "\n",
    "    dir_name = 'temp'\n",
    "    mkdir_new(dir_name)\n",
    "    # tables = camelot.read_pdf('C++.pdf', pages='2-end')\n",
    "    tables.export('{}/{}'.format(dir_name, 'temp.csv'), f='csv', compress=False)\n",
    "    # iterate across all produced csvs and produce rst or tex\n",
    "    for file_name in list(glob.glob(\"temp/*.csv\")):\n",
    "        if args.rst:\n",
    "            # consider saving rst in different folder \n",
    "            rst_name = '{}.{}'.format(splitext(file_name)[0], 'rst')\n",
    "            to_rst(file_name, rst_name)\n",
    "        if args.tex:\n",
    "            tex_name = '{}.{}'.format(splitext(file_name)[0], 'tex')\n",
    "            to_tex(file_name, tex_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-p\", \n",
    "                        \"--pdf\", \n",
    "                        help=\"Pdf Document with pages\", \n",
    "                        default=\"C++.pdf\") \n",
    "    parser.add_argument(\"-pa\", \n",
    "                        \"--pages\", \n",
    "                        help=\"Pdf document Pages\",\n",
    "                        default=\"2-end\") \n",
    "    parser.add_argument(\"-rst\", \n",
    "                      \"--rst_tables\", \n",
    "                      help=\"Take outputted csvs and convert to restructured text using pandoc\",\n",
    "                      default=\"Bad Password\")\n",
    "    parser.add_argument('--rst', dest='rst', action='store_true')\n",
    "    parser.add_argument('--no-rst', dest='rst', action='store_false')\n",
    "    parser.add_argument('--feature', dest='feature', action='store_true')\n",
    "    parser.add_argument('--no-feature', dest='feature', action='store_false')\n",
    "    parser.add_argument(\"-tex\", \n",
    "                      \"--latex\", \n",
    "                      help=\"Take outputted csvs and convert to latex using pandas\",\n",
    "                      action=\"store_true\"\n",
    "                      ) \n",
    "    parser.set_defaults(rst=True, tex=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)\n",
    "```\n",
    "\n",
    "The following utility scripts uses camelot to extract tables from pdf and then save them in `rst` format, it has difficult merging tables split around 2 pages which is why I save the individual `csvs` files.\n",
    "\n",
    "Some sample output can be seen at {ref}`some_value_factors`, {numref}`some_value_factors`\n",
    "\n",
    "\n",
    "Another script that I have used is for converting glossary for the latex format to something that jupyterbook can understand, the script is very rough and does not work with references to terms within terms. `\\glsapi{}` will be inputted as raw latex, I am expecting the excecutable book project to eventually produce a better solution, but in the interim this script looks for my purposes.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Converts a tex glossary into a format jupyterbook can understand\n",
    "\n",
    "Extended from https://tex.stackexchange.com/questions/529971/how-do-i-sort-all-of-the-glossary-entries-in-a-tex-file\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import webbrowser\n",
    "from collections import OrderedDict\n",
    "\n",
    "commands = (r\"\\\\newglossaryentry\", r\"\\\\newabbreviation\", r\"\\\\newacronym\")\n",
    "startcmd_regex = re.compile(\n",
    "    r\"(?:\" + r\"|\".join(commands) + r\")\\s*\\r?\\n?\\s*\\{(.+?)\\}\\s*\\r?\\n?\\s*\\{\"\n",
    ")\n",
    "brace_regex = re.compile(r\"(?<!\\\\)(?:\\\\\\\\)*(\\{|\\})\")\n",
    "\n",
    "\n",
    "class UnreachableError(Exception):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        webbrowser.open(\"https://xkcd.com/2200/\")\n",
    "        super().__init__(\n",
    "            \"The code is in what I thought was an unreachable state.\", *args, **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "class ParseError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def scan_file(fname):\n",
    "    \"\"\"\n",
    "    Scan a file and separate it into entries and outer code.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        The file name.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ParseError\n",
    "        If the end of an entry cannot be found.\n",
    "    UnreachableError\n",
    "        If a state is reached that should be unreachable.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    entries : dict\n",
    "        The entries.\n",
    "        entries[None] always containes the collected outer code.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(fname, \"r\", errors=\"ignore\") as rf:\n",
    "        text = rf.read()\n",
    "    open_braces = 0\n",
    "    entries = {None: \"\"}\n",
    "    label = None\n",
    "    while text:\n",
    "        if label is None:\n",
    "            # Look for next entry.\n",
    "            match = startcmd_regex.search(text)\n",
    "            if match is None:\n",
    "                piece = text.strip()\n",
    "                if piece:\n",
    "                    entries[label] += piece + \"\\n\"\n",
    "                text = \"\"\n",
    "            else:\n",
    "                piece = text[0 : match.start()].strip()\n",
    "                if piece:\n",
    "                    entries[label] += piece + \"\\n\"\n",
    "                label = match.group(1)\n",
    "                if label in entries:\n",
    "                    entries[label] += \"\\n\"\n",
    "                else:\n",
    "                    entries[label] = \"\"\n",
    "                entries[label] += match.group(0).lstrip()\n",
    "                open_braces = 1\n",
    "                text = text[match.end() :]\n",
    "        else:\n",
    "            # Look for next brace.\n",
    "            match = brace_regex.search(text)\n",
    "            if match is None:\n",
    "                raise ParseError(\n",
    "                    \"I could not find all closing braces in \"\n",
    "                    + \"'{}' ({} open braces)!\".format(label, open_braces)\n",
    "                )\n",
    "            else:\n",
    "                entries[label] += text[: match.end()]\n",
    "                text = text[match.end() :]\n",
    "                brace = match.group(1)\n",
    "                if brace == \"{\":\n",
    "                    open_braces += 1\n",
    "                elif brace == \"}\":\n",
    "                    open_braces -= 1\n",
    "                else:\n",
    "                    raise UnreachableError\n",
    "                if open_braces == 0:\n",
    "                    # This entry is done.\n",
    "                    label = None\n",
    "                elif open_braces < 0:\n",
    "                    raise UnreachableError\n",
    "    return entries\n",
    "\n",
    "\n",
    "def sort_entries(source, *, label_sort=False):\n",
    "    \"\"\"\n",
    "    Sort all the entries in the source file and returns and dict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source : str\n",
    "        The name of the source file.\n",
    "    label_sort : bool, optional\n",
    "        If True, the entries are sorted by their label, otherwise they are\n",
    "        sorted by their definition (including the definition command).\n",
    "        The default is False.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the source file cannot be found.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    present_files = os.listdir()\n",
    "    if source not in present_files:\n",
    "        raise ValueError(\"I cannot find the file '{}'!\".format(source))\n",
    "    entries = scan_file(source)\n",
    "    return entries\n",
    "\n",
    "\n",
    "def dict_with_descr(entries: dict):\n",
    "    new_entries = {}\n",
    "    for key in entries:\n",
    "        if key == None:\n",
    "            continue\n",
    "        else:\n",
    "            glossary_entry = entries[key]\n",
    "            split_content = glossary_entry.split(\"description\", 1)[1]\n",
    "            # replace { } and \\t \\n\n",
    "            cleaned_content = re.sub(\"{|}|\\\\t|\\\\n|=\", \"\", split_content)\n",
    "            # to extract words from string\n",
    "            description = cleaned_content\n",
    "            new_entries[key] = description\n",
    "\n",
    "    return new_entries\n",
    "\n",
    "\n",
    "def make_glossary(descriptions: dict):\n",
    "    gloss_open = r\"```{glossary}\"\n",
    "    gloss_end = r\"```\"\n",
    "    body = \"\"\n",
    "    # sort keys first\n",
    "    for desc_key in sorted(descriptions.keys()):\n",
    "        body = body + f\"{desc_key}\\n\\t{descriptions[desc_key]}\"\n",
    "        body = body + \"\\n\\n\"\n",
    "    return f\"{gloss_open}\\n{body}\\n{gloss_end}\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Sort the glossary entries in a file.\")\n",
    "    parser.add_argument(\"-l\", \"--label-sort\", action=\"store_true\")\n",
    "    parser.add_argument(\"source\", help=\"The source file.\")\n",
    "    args = parser.parse_args()\n",
    "    entries = sort_entries(args.source, label_sort=args.label_sort)\n",
    "    descriptions = dict_with_descr(entries)\n",
    "    glossary_str = make_glossary(descriptions)\n",
    "    with open('glossary_data.md', 'w') as _f:\n",
    "        _f.write(glossary_str)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "For an example glossary file with the contents \n",
    "\n",
    "\n",
    "```tex\n",
    "\\newglossaryentry{DebtToEquity}\n",
    "{\n",
    "\tname={Debt To Equity},\n",
    "\tdescription={\n",
    "\t\tThe debt-to-equity (D/E) ratio indicates how much debt a company is using to finance its assets relative to the value of shareholders' equity.\n",
    "\t}\n",
    "}\n",
    "\n",
    "\\newglossaryentry{DeadCatBounce}\n",
    "{\n",
    "\tname={Dead Cat Bounce},\n",
    "\tdescription={\n",
    "\t\tDead cat bounce is a small, brief recovery in the price of a declining stock.\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "We would expect this script to output, this is really only useful if you are seeking to produce/convert jupyterbook content.\n",
    "\n",
    "```tex\n",
    "{glossary}\n",
    "DeadCatBounce\n",
    "        Dead cat bounce is a small, brief recovery in the price of a declining stock.\n",
    "\n",
    "DebtToEquity\n",
    "        The debt-to-equity (D/E) ratio indicates how much debt a company is using to finance its assets relative to the value of shareholders' equity.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{toctree}\n",
    ":hidden:\n",
    ":titlesonly:\n",
    "\n",
    "\n",
    "Performance analyze as of 06/01/2020 <../notebooks/analyze/Performance_06-01-2020>\n",
    "Risk Analyze 06/05/2020 <../notebooks/analyze/RISK_06-05-2020>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "source_map": [
   10
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}